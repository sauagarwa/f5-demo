device: gpu
rawDeploymentMode: true

servingRuntime:
  name: vllm-serving-runtime
  knativeTimeout: 60m
  image: quay.io/ecosystem-appeng/vllm:openai-v0.8.5
  recommendedAccelerators:
    - nvidia.com/gpu
  env:
    - name: HOME
      value: /vllm
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          key: HF_TOKEN
          name: huggingface-secret
  volumeMounts:
    - name: shm
      mountPath: /dev/shm
    - name: vllm-chat-templates
      mountPath: /chat-templates
    - name: vllm-home
      mountPath: /vllm
  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi
    - name: vllm-chat-templates
      configMap:
        name: vllm-chat-templates
    - name: vllm-home
      emptyDir: {}

secret:
  enabled: true
  hf_token: ""

models:
  llama-3-2-1b-instruct:
    id: meta-llama/Llama-3.2-1B-Instruct
    enabled: false
    # A default "nvidia.com/gpu" toleration is implemented in the
    # inference-service.yaml template and can be overriden as follows:
    # tolerations:
    #   - key: nvidia.com/gpu
    #     effect: NoSchedule
    #     operator: Exists
    args:
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"

  llama-3-2-3b-instruct:
    id: meta-llama/Llama-3.2-3B-Instruct
    enabled: false
    args:
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"


  llama-3-1-8b-instruct:
    id: meta-llama/Llama-3.1-8B-Instruct
    enabled: false
    resources:
      limits:
        nvidia.com/gpu: "1"
    args:
      - --max-model-len
      - "14336"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json

  llama-3-2-1b-instruct-quantized:
    id: RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8
    enabled: false
    args:
      - --gpu-memory-utilization
      - "0.4"
      - --quantization
      - compressed-tensors
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"
