device: gpu
rawDeploymentMode: true

servingRuntime:
  name: vllm-serving-runtime
  knativeTimeout: 60m
  image: quay.io/ecosystem-appeng/vllm:openai-v0.8.5
  recommendedAccelerators:
    - nvidia.com/gpu
  env:
    - name: HOME
      value: /vllm
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          key: HF_TOKEN
          name: huggingface-secret
  volumeMounts:
    - name: shm
      mountPath: /dev/shm
    - name: vllm-chat-templates
      mountPath: /chat-templates
    - name: vllm-home
      mountPath: /vllm
  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi
    - name: vllm-chat-templates
      configMap:
        name: vllm-chat-templates
    - name: vllm-home
      emptyDir: {}

secret:
  enabled: true
  hf_token: ""

models:
  granite-3-3-2b-instruct:
    id: ibm-granite/granite-3.3-2b-instruct
    enabled: true
    resources:
      limits:
        nvidia.com/gpu: "1"
    # If the gpu nodes are tainted, specify the correct toleration
    # to provision the models on the tainted nodes
    # tolerations:
    #   - key: nvidia.com/gpu
    #     effect: NoSchedule
    #     operator: Exists
    args:
      - --tensor-parallel-size
      - "1"
      - --max-model-len
      - "8192"
      - --enable-auto-tool-choice
      - --tool-call-parser
      - granite

  llama-3-2-1b-instruct-quantized:
    id: RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8
    enabled: true
    # If the gpu nodes are tainted, specify the correct toleration
    # to provision the models on the tainted nodes
    # tolerations:
    #   - key: nvidia.com/gpu
    #     effect: NoSchedule
    #     operator: Exists
    args:
      - --gpu-memory-utilization
      - "0.4"
      - --quantization
      - compressed-tensors
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "8192"
